# The answer of Problem 5 will be written into answerFile.txt
answerFile = open("answerFile.txt","w")

### PART 1 ###
# From the original ratings.csv we generate an RDD, after which a new RDD
# containing only movieID of each rating is generated. countByValue() gives the
# number of ratings of each movie. By summing the values up and divide by the
# number of keys in the output of countByValue(), we get answer 1.

import csv
from pyspark import SparkContext
sc = SparkContext()

results = []
with open("ratings.csv") as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
        results.append(row)

myrdd = sc.parallelize(results) # entry in this RDD: (userId, movieId, rating, timestamp)
newrdd = myrdd.map(lambda x:x[1]) # entry in this RDD: (movieId)

rateNumDict = newrdd.countByValue() # key: movieId, value: number of ratings of the movie
totalRateCnt = sum(rateNumDict.values()) # counting total number of ratings in the data set
answer1 = totalRateCnt / len(rateNumDict) # answer to part 1: total number of ratings / total number of movies

answerFile.write("Answer 1: \n" + str(answer1))

### PART 2 ###
# The average rating of each genre is obtained by total ratings of movies in that
# genre divided by the number of rates in that genre. We first computed the total
# rating of each movie by mapping to (movieId, rating) and reduceByKey using
# summation. Then, for movies.csv, we decomposed the genre for each movie and
# created an Rdd with (movie id, genre), after which joined by movie id. The
# total ratings of each genre then can be generated by mapping to (genre, rating
# to one movie) and reducebykey using summation. The number of ratings in each
# genre can be generated by mapping to (genre,id) and countByValue(). Combining
# above information by join function, we can compute the average rating of each
# genre by dividing the total rating of each genre with the number of rating in
# the genre.

ratingrdd = myrdd.map(lambda x:(x[1],float(x[2]))) # entry in this RDD: (movieId, rating)
totalRatingRdd = ratingrdd.reduceByKey(lambda x,y: x+y) # entry in this RDD: (movieId, sum of its rating)

movies = [] # input movies.csv

with open("movies.csv") as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
        movies.append(row)

def decompose(mylist):
	output = []
	genrelist = mylist[2].split('|')
	for i in range(len(genrelist)):
		output.append([genrelist[i],mylist[0]])
	return output

movrdd = sc.parallelize(movies).flatMap(decompose) # movrdd: each entry is (genre, movie id)
# GenreNum*** is to count how many movies are rated in each genre

IdGenreRateJoinedRdd = movrdd.map(lambda x:[x[1],x[0]]).join(ratingrdd) # after join by id we have (id,(genre,rate))
GenreNumDict = IdGenreRateJoinedRdd.map(lambda x:[x[1][0],x[0]]).countByKey() # after map we have (genre,id)

# convert dictionary to list to be parallelized in spark
def DicToList(dic):
	temp = []
	for key,value in dic.items():
		temp.append([key,value])
	return temp

# GenreNumDict has key-value pairs {genre: number of rates to this genre}
# convert dictionary to list to be parallelized in spark
GenreNumRdd = sc.parallelize(DicToList(GenreNumDict))

# to avoid using expensive functions like collect, we need to join totalRatingRdd with movrdd by the movieID
# switch the key and value in movrdd then combine with totalRatingRdd
MovRateJoinedRdd = movrdd.map(lambda x:[x[1],x[0]]).join(totalRatingRdd) # MovRateJoinedRdd: entry is (movie id,(genre, total rate of the movie id))

# get an rdd with genre and  and then take the sum
GenreMovRateRdd = MovRateJoinedRdd.map(lambda x:[x[1][0],x[1][1]]) # GenreMovRateRDD: enrey is (genre, total rate of the movie)
GenreTotalRateRdd = GenreMovRateRdd.reduceByKey((lambda x,y: x + y))
# ANSWER2 # after join we have (genre,(total rate of genre, num of rates))
GenreAveRatingRdd = GenreTotalRateRdd.join(GenreNumRdd).map(lambda x:[x[0],float(x[1][0]) / float(x[1][1])]) # average rating of each genre = total rating of the genre divided by the number of movies in that genre

answerFile.write("\n\nAnswer 2: " )
for i in GenreAveRatingRdd.collect():
    answerFile.write("\n" + i[0] + ": "+ str(i[1]))

### PART3 ###
# With the help of results from PART1 and PART2, we can join the rdd(MovIdRateNumRd)
# that has the number of rates to each movie with the rdd(MovRateJoinedRdd) that has
# total ratings of each movie, and then divide them to get the average rating of each
# movie. Note along with the computation, we have genre label preserved. By mapping
# the genre as the key, (movie id, average rating to this movie) as the value, we can
# use groupbykey to get the movies and their average rating under this genre. Then we
# created a new rdd with the lists of (movie id, average rating to this movie) for each
# genre and used top() function to output top3 movies with top3 average ratings in each
# genre.

MovIdRateNumRdd = sc.parallelize(DicToList(rateNumDict))
GenreMovIdAvg = MovIdRateNumRdd.join(MovRateJoinedRdd).map(lambda x:[x[1][1][0],[x[0],float(x[1][1][1]) / float(x[1][0])]]) # GenreMovIdAvg: the entry is (genre, (movid, avg rating of movid)

rddGenreArrayOfMovies = GenreMovIdAvg.groupByKey() # entry in this RDD: (genreName, [[movieId, average rating of the movie]...])
listGenreArrayOfMovies = rddGenreArrayOfMovies.collect() # [[genreName, [[movieId, average rating of the movie]...]]...]
answerThreeArray = []
for eachGenre in listGenreArrayOfMovies:
    topMovies = sc.parallelize(eachGenre[1]).top(3, key=lambda x: x[1])
    answerThreeArray.append(["Top movies under " + eachGenre[0] + ":",topMovies])

movieIdMovieNameRdd = sc.parallelize(movies).map(lambda x: [x[0], x[1]]) # entry in this RDD: (movieId, movieName)

answerFile.write("\n\nAnswer 3: " )
for i in answerThreeArray:
    answerFile.write("\n" + i[0] + " ")
    for j in i[1]:
        answerFile.write(movieIdMovieNameRdd.lookup(j[0])[0] + " (Rating: " + str(j[1]) + ") | ") # using lookup() function to get movieName by movieId

### PART 4 ###
# For top 10 watchers, we just need to count how many userIDs appeared in
# the ratings.csv file. This is done by mapping to (userID) and countByValue().
# Then with sorted dictionary we can find top10 watchers and their ID in the
# dictionary.

#ANSWER4
userDict = myrdd.map(lambda x:x[0]).countByValue()
top10UsersRate = sorted(userDict.values())[-10:]
inv_userDict = {v: k for k, v in userDict.items()}
top10UsersID = [] # ANSWER4
for i in range(10):
	top10UsersID.append(inv_userDict.get(top10UsersRate[9 - i])) # order is from user who rated the most to the least

answerFile.write("\n\nAnswer 4: \n" )
for i in top10UsersID:
    answerFile.write(i + " ")

### PART 5 ###
# Firstly, we defined 3 functions. mergeString will take in two arguments, either
# string or list of strings and then merge them into a combined list of strings.
# combination function will take in a list and output a list with all possible
# combinations/pairs of elements in the input list, certain order applied for
# comparison reason. splitcombination takes in a list of combinations and outputs
# each single pair as key and corresponding movie id as value. The algorithm is like
# this: we map the rating.csv to (movie id, user id) then reducebykey using mergeString
# function and output will be (movie id, (userid_1, userid_2,...,userid_n)). Then
# we create combinations on the value, i.e. the list of users that have rated the movie,
# and output new key-value pairs as ((userid_1,userid_2),movie id), ((userid_1,userid_3),movie id)
# using splitcombination function. Now countByValue() will give us the number movies
# that a pair both have rated. Again, with sorted dictionary we can output top10 pairs
# with their user ids.

def mergeString(x,y):
	if isinstance(x,str) and isinstance(y,str):
		return [x,y]
	elif isinstance(x,str) and isinstance(y,list):
		y.append(x)
		return y
	elif isinstance(x,list) and isinstance(y,str):
		x.append(y)
		return x
	elif isinstance(x,list) and isinstance(y,list):
		return x + y

def combination(l):
	temp = []
	mylist = l[1]
	if isinstance(mylist,list):
		for i in range(len(mylist) - 1):
			for j in range(i + 1,len(mylist)):
				if mylist[i] <= mylist[j]: # fix the order for later comparison
					temp.append([mylist[i],mylist[j]])
				else:
					temp.append([mylist[j],mylist[i]])
	return [l[0],temp]

def splitcombination(l):
	temp = []
	for i in range(len(l[1])):
		temp.append([l[1][i],l[0]])
	return temp

def getKeybyValue(mydic, myval):
	mylist = []
	for i in mydic.items():
		if i[1] == myval:
			mylist.append(i[0])
	return mylist

# ANSWER5

myrdd = sc.parallelize(results).map(lambda x:[x[1],x[0]])
MovUserCombinationRdd = myrdd.reduceByKey(mergeString).map(combination) # MovUserCombinationRdd: entry is (movid,(all combinations of movie))
pairDict = MovUserCombinationRdd.flatMap(splitcombination).map(lambda x: (tuple(x[0]), x[1])).countByKey() # after splitting we have ((pairs of users), one movie that they both watched). After map we have only (pair of users)

top10PairID = [] # ANSWER5
sortedPairDict = sorted(pairDict.items(), key=lambda x: x[1], reverse=True) # sort the dictionary by value in descending order
for i in sortedPairDict[:10]:
	top10PairID.append(i) # get key from the dictionary the top 10 users with order from user who rated the most to the least
answerFile.write("\n\nAnswer 5: \n" )
for i in top10PairID:
	answerFile.write("(" + i[0][0] + " " + i[0][1] + ") : "+ str(i[1]) + " | ")

answerFile.close()
